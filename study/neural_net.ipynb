{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8020607\n",
      "1000 0.6931816\n",
      "2000 0.6931472\n",
      "3000 0.6931472\n",
      "4000 0.6931472\n",
      "5000 0.6931472\n",
      "6000 0.6931472\n",
      "7000 0.6931472\n",
      "8000 0.6931472\n",
      "9000 0.6931472\n",
      "10000 0.6931472\n",
      "\n",
      "Hypothesis:  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "# Lab 9 XOR\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7484501\n",
      "1000 0.65588176\n",
      "2000 0.4418475\n",
      "3000 0.1546641\n",
      "4000 0.07062855\n",
      "5000 0.04341809\n",
      "6000 0.030849116\n",
      "7000 0.023760619\n",
      "8000 0.019252025\n",
      "9000 0.016147183\n",
      "10000 0.0138857495\n",
      "\n",
      "Hypothesis:  [[0.01659706]\n",
      " [0.987468  ]\n",
      " [0.98747325]\n",
      " [0.01349006]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name=\"weight1\") #watch out for shape\n",
    "b1 = tf.Variable(tf.random_normal([2]), name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1) #layer up\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]), name=\"bias2\")\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2) \n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8718714\n",
      "1000 0.63003504\n",
      "2000 0.1484763\n",
      "3000 0.019048937\n",
      "4000 0.008573602\n",
      "5000 0.0053317314\n",
      "6000 0.0038119634\n",
      "7000 0.0029435845\n",
      "8000 0.0023861441\n",
      "9000 0.001999984\n",
      "10000 0.0017176009\n",
      "\n",
      "Hypothesis:  [[0.00134108]\n",
      " [0.9979468 ]\n",
      " [0.99852884]\n",
      " [0.00199789]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1') #input layer\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 10]), name='weight2') #hidden layer 1\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10, 10]), name='weight3') #hidden layer 2\n",
    "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([10, 1]), name='weight4') #output layer\n",
    "b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "#more deeper, more correctness\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n",
    "    #more accurate hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001, Cost: 2.225186107\n",
      "Epoch: 0002, Cost: 1.905111210\n",
      "Epoch: 0003, Cost: 1.620166732\n",
      "Epoch: 0004, Cost: 1.364068823\n",
      "Epoch: 0005, Cost: 1.163689887\n",
      "Epoch: 0006, Cost: 1.022326402\n",
      "Epoch: 0007, Cost: 0.928538522\n",
      "Epoch: 0008, Cost: 0.864302507\n",
      "Epoch: 0009, Cost: 0.816096415\n",
      "Epoch: 0010, Cost: 0.776680620\n",
      "Epoch: 0011, Cost: 0.743331950\n",
      "Epoch: 0012, Cost: 0.714563267\n",
      "Epoch: 0013, Cost: 0.689446815\n",
      "Epoch: 0014, Cost: 0.666299741\n",
      "Epoch: 0015, Cost: 0.644688191\n",
      "Epoch: 0016, Cost: 0.624602862\n",
      "Epoch: 0017, Cost: 0.605484639\n",
      "Epoch: 0018, Cost: 0.587318824\n",
      "Epoch: 0019, Cost: 0.570870001\n",
      "Epoch: 0020, Cost: 0.555452932\n",
      "Epoch: 0021, Cost: 0.541397862\n",
      "Epoch: 0022, Cost: 0.528265215\n",
      "Epoch: 0023, Cost: 0.516588970\n",
      "Epoch: 0024, Cost: 0.505570135\n",
      "Epoch: 0025, Cost: 0.495472980\n",
      "Epoch: 0026, Cost: 0.486129417\n",
      "Epoch: 0027, Cost: 0.477535171\n",
      "Epoch: 0028, Cost: 0.469369619\n",
      "Epoch: 0029, Cost: 0.461728129\n",
      "Epoch: 0030, Cost: 0.454610681\n",
      "Epoch: 0031, Cost: 0.448115404\n",
      "Epoch: 0032, Cost: 0.441685830\n",
      "Epoch: 0033, Cost: 0.435512743\n",
      "Epoch: 0034, Cost: 0.429743052\n",
      "Epoch: 0035, Cost: 0.424245987\n",
      "Epoch: 0036, Cost: 0.419136461\n",
      "Epoch: 0037, Cost: 0.414135784\n",
      "Epoch: 0038, Cost: 0.409647218\n",
      "Epoch: 0039, Cost: 0.404898654\n",
      "Epoch: 0040, Cost: 0.401168784\n",
      "Epoch: 0041, Cost: 0.397046306\n",
      "Epoch: 0042, Cost: 0.393536129\n",
      "Epoch: 0043, Cost: 0.389903228\n",
      "Epoch: 0044, Cost: 0.386540882\n",
      "Epoch: 0045, Cost: 0.383297111\n",
      "Epoch: 0046, Cost: 0.380150249\n",
      "Epoch: 0047, Cost: 0.376947967\n",
      "Epoch: 0048, Cost: 0.374237116\n",
      "Epoch: 0049, Cost: 0.371116335\n",
      "Epoch: 0050, Cost: 0.368443826\n",
      "Epoch: 0051, Cost: 0.365839454\n",
      "Epoch: 0052, Cost: 0.363056375\n",
      "Epoch: 0053, Cost: 0.360937522\n",
      "Epoch: 0054, Cost: 0.358383524\n",
      "Epoch: 0055, Cost: 0.355882641\n",
      "Epoch: 0056, Cost: 0.353853652\n",
      "Epoch: 0057, Cost: 0.351377720\n",
      "Epoch: 0058, Cost: 0.349331733\n",
      "Epoch: 0059, Cost: 0.347172316\n",
      "Epoch: 0060, Cost: 0.345321018\n",
      "Epoch: 0061, Cost: 0.343577547\n",
      "Epoch: 0062, Cost: 0.341360275\n",
      "Epoch: 0063, Cost: 0.339548872\n",
      "Epoch: 0064, Cost: 0.337844349\n",
      "Epoch: 0065, Cost: 0.336079723\n",
      "Epoch: 0066, Cost: 0.334322022\n",
      "Epoch: 0067, Cost: 0.332774467\n",
      "Epoch: 0068, Cost: 0.330965750\n",
      "Epoch: 0069, Cost: 0.329361192\n",
      "Epoch: 0070, Cost: 0.327510333\n",
      "Epoch: 0071, Cost: 0.326155733\n",
      "Epoch: 0072, Cost: 0.324362686\n",
      "Epoch: 0073, Cost: 0.322812426\n",
      "Epoch: 0074, Cost: 0.321122276\n",
      "Epoch: 0075, Cost: 0.319652765\n",
      "Epoch: 0076, Cost: 0.318392322\n",
      "Epoch: 0077, Cost: 0.316776656\n",
      "Epoch: 0078, Cost: 0.315330774\n",
      "Epoch: 0079, Cost: 0.313955686\n",
      "Epoch: 0080, Cost: 0.312914091\n",
      "Epoch: 0081, Cost: 0.311293713\n",
      "Epoch: 0082, Cost: 0.309764962\n",
      "Epoch: 0083, Cost: 0.308388524\n",
      "Epoch: 0084, Cost: 0.307215845\n",
      "Epoch: 0085, Cost: 0.306190724\n",
      "Epoch: 0086, Cost: 0.304799596\n",
      "Epoch: 0087, Cost: 0.303782424\n",
      "Epoch: 0088, Cost: 0.302452804\n",
      "Epoch: 0089, Cost: 0.301341204\n",
      "Epoch: 0090, Cost: 0.300381863\n",
      "Epoch: 0091, Cost: 0.299060335\n",
      "Epoch: 0092, Cost: 0.297852151\n",
      "Epoch: 0093, Cost: 0.296884203\n",
      "Epoch: 0094, Cost: 0.295755031\n",
      "Epoch: 0095, Cost: 0.294769320\n",
      "Epoch: 0096, Cost: 0.293561734\n",
      "Epoch: 0097, Cost: 0.292517936\n",
      "Epoch: 0098, Cost: 0.291364737\n",
      "Epoch: 0099, Cost: 0.290687562\n",
      "Epoch: 0100, Cost: 0.289621522\n",
      "Epoch: 0101, Cost: 0.288696326\n",
      "Epoch: 0102, Cost: 0.287760850\n",
      "Epoch: 0103, Cost: 0.286988585\n",
      "Epoch: 0104, Cost: 0.285944757\n",
      "Epoch: 0105, Cost: 0.284756335\n",
      "Epoch: 0106, Cost: 0.284217557\n",
      "Epoch: 0107, Cost: 0.283337554\n",
      "Epoch: 0108, Cost: 0.282239053\n",
      "Epoch: 0109, Cost: 0.281478147\n",
      "Epoch: 0110, Cost: 0.280845371\n",
      "Epoch: 0111, Cost: 0.279803062\n",
      "Epoch: 0112, Cost: 0.279022281\n",
      "Epoch: 0113, Cost: 0.278075751\n",
      "Epoch: 0114, Cost: 0.277215604\n",
      "Epoch: 0115, Cost: 0.276390548\n",
      "Epoch: 0116, Cost: 0.275570553\n",
      "Epoch: 0117, Cost: 0.274834205\n",
      "Epoch: 0118, Cost: 0.274254765\n",
      "Epoch: 0119, Cost: 0.273558373\n",
      "Epoch: 0120, Cost: 0.272551496\n",
      "Epoch: 0121, Cost: 0.271819186\n",
      "Epoch: 0122, Cost: 0.271309118\n",
      "Epoch: 0123, Cost: 0.270534820\n",
      "Epoch: 0124, Cost: 0.269554332\n",
      "Epoch: 0125, Cost: 0.269139929\n",
      "Epoch: 0126, Cost: 0.268523207\n",
      "Epoch: 0127, Cost: 0.267884376\n",
      "Epoch: 0128, Cost: 0.267103053\n",
      "Epoch: 0129, Cost: 0.266074023\n",
      "Epoch: 0130, Cost: 0.265907076\n",
      "Epoch: 0131, Cost: 0.265050625\n",
      "Epoch: 0132, Cost: 0.264469110\n",
      "Epoch: 0133, Cost: 0.263549288\n",
      "Epoch: 0134, Cost: 0.262988772\n",
      "Epoch: 0135, Cost: 0.262424789\n",
      "Epoch: 0136, Cost: 0.261975688\n",
      "Epoch: 0137, Cost: 0.261221837\n",
      "Epoch: 0138, Cost: 0.260517010\n",
      "Epoch: 0139, Cost: 0.259742109\n",
      "Epoch: 0140, Cost: 0.259560239\n",
      "Epoch: 0141, Cost: 0.258442702\n",
      "Epoch: 0142, Cost: 0.258186721\n",
      "Epoch: 0143, Cost: 0.257643994\n",
      "Epoch: 0144, Cost: 0.257026117\n",
      "Epoch: 0145, Cost: 0.256504965\n",
      "Epoch: 0146, Cost: 0.255819155\n",
      "Epoch: 0147, Cost: 0.255435659\n",
      "Epoch: 0148, Cost: 0.254665846\n",
      "Epoch: 0149, Cost: 0.254186811\n",
      "Epoch: 0150, Cost: 0.253704370\n",
      "Accuracy:  0.9134\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_calses = 10\n",
    "\n",
    "X=tf.placeholder(tf.float32, [None, 784]) #28*28 image\n",
    "Y=tf.placeholder(tf.float32, [None, nb_calses])\n",
    "\n",
    "W1=tf.Variable(tf.random_normal([784, 10]))\n",
    "b1=tf.Variable(tf.random_normal([10]))\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2=tf.Variable(tf.random_normal([10, 10]))\n",
    "b2=tf.Variable(tf.random_normal([10]))\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3=tf.Variable(tf.random_normal([10, 10]))\n",
    "b3=tf.Variable(tf.random_normal([10]))\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4=tf.Variable(tf.random_normal([10, nb_calses]))\n",
    "b4=tf.Variable(tf.random_normal([nb_calses]))\n",
    "hypothesis = tf.nn.softmax(tf.matmul(layer3,W4) + b4)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis,1), tf.arg_max(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "training_epochs = 150\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "    print(\"Accuracy: \",accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
